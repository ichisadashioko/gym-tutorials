{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, num_states, num_actions, alpha=0.2, gamma=0.8):\n",
    "        \"\"\"\n",
    "        `alpha`: learning rate\n",
    "\n",
    "        `gamma`: discount factor\n",
    "        \"\"\"\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        # Initialize Q table with 0\n",
    "        self.q_table = np.zeros((num_states, num_actions), dtype=np.float)\n",
    "\n",
    "    def update_table(self, state, action, reward, new_state):\n",
    "        # self.q_table[state, action] = self.q_table[state, action] - self.alpha * (reward + self.gamma * np.max(self.q_table[new_state]) - self.q_table[state, action])\n",
    "\n",
    "        # or\n",
    "        self.q_table[state, action] = (1 - self.alpha) * self.q_table[state, action] + \\\n",
    "            self.alpha * (reward + self.gamma *\n",
    "                          np.max(self.q_table[new_state]))\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        return np.argmax(self.q_table[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register an un-slippery version of `FrozenLake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_states = [5, 7, 11, 12]\n",
    "goal_states = [15]\n",
    "\n",
    "hole_reward = -10\n",
    "goal_reward = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100 -------\n",
      "Episode 2/100 ----------\n",
      "Episode 3/100 ----------\n",
      "Episode 4/100 --------\n",
      "Episode 5/100 --\n",
      "Episode 6/100 ------------\n",
      "Episode 7/100 ------\n",
      "Episode 8/100 ----------------\n",
      "Episode 9/100 --\n",
      "Episode 10/100 ----\n",
      "Episode 11/100 ----------------\n",
      "Episode 12/100 -----\n",
      "Episode 13/100 --\n",
      "Episode 14/100 ----------------\n",
      "Episode 15/100 --\n",
      "Episode 16/100 ---------------\n",
      "Episode 17/100 ------\n",
      "Episode 18/100 -----\n",
      "Episode 19/100 ----\n",
      "Episode 20/100 --------\n",
      "Episode 21/100 ---------\n",
      "Episode 22/100 ------\n",
      "Episode 23/100 --\n",
      "Episode 24/100 --\n",
      "Episode 25/100 --\n",
      "Episode 26/100 --------\n",
      "Episode 27/100 ----\n",
      "Episode 28/100 ------\n",
      "Episode 29/100 ------------\n",
      "Episode 30/100 --------------\n",
      "Episode 31/100 ----\n",
      "Episode 32/100 --\n",
      "Episode 33/100 ---------------\n",
      "Episode 34/100 -----------\n",
      "Episode 35/100 --\n",
      "Episode 36/100 --\n",
      "Episode 37/100 --------\n",
      "Episode 38/100 ----\n",
      "Episode 39/100 -----\n",
      "Episode 40/100 -----\n",
      "Episode 41/100 --------------\n",
      "Episode 42/100 ---------\n",
      "Episode 43/100 --------\n",
      "Episode 44/100 -----\n",
      "Episode 45/100 --------\n",
      "Episode 46/100 --------------\n",
      "Episode 47/100 -----\n",
      "Episode 48/100 ---\n",
      "Episode 49/100 ----------\n",
      "Episode 50/100 --\n",
      "Episode 51/100 -------\n",
      "Episode 52/100 --\n",
      "Episode 53/100 ---------\n",
      "Episode 54/100 ------\n",
      "Episode 55/100 ------\n",
      "Episode 56/100 --\n",
      "Episode 57/100 ----------\n",
      "Episode 58/100 -----\n",
      "Episode 59/100 --------\n",
      "Episode 60/100 ------\n",
      "Episode 61/100 -------\n",
      "Episode 62/100 -------\n",
      "Episode 63/100 ------\n",
      "Episode 64/100 ----\n",
      "Episode 65/100 -------\n",
      "Episode 66/100 -----\n",
      "Episode 67/100 --\n",
      "Episode 68/100 ---\n",
      "Episode 69/100 ----------\n",
      "Episode 70/100 -------\n",
      "Episode 71/100 ------\n",
      "Episode 72/100 -------\n",
      "Episode 73/100 -----\n",
      "Episode 74/100 ------\n",
      "Episode 75/100 ------\n",
      "Episode 76/100 ------------\n",
      "Episode 77/100 --\n",
      "Episode 78/100 -----\n",
      "Episode 79/100 ------\n",
      "Episode 80/100 --------\n",
      "Episode 81/100 ------\n",
      "Episode 82/100 --------\n",
      "Episode 83/100 ------\n",
      "Episode 84/100 ------\n",
      "Episode 85/100 ------\n",
      "Episode 86/100 ------\n",
      "Episode 87/100 ------\n",
      "Episode 88/100 --------\n",
      "Episode 89/100 ------\n",
      "Episode 90/100 ---\n",
      "Episode 91/100 ------\n",
      "Episode 92/100 -------\n",
      "Episode 93/100 ------\n",
      "Episode 94/100 ------\n",
      "Episode 95/100 ------\n",
      "Episode 96/100 ------\n",
      "Episode 97/100 ------\n",
      "Episode 98/100 ------\n",
      "Episode 99/100 ------\n",
      "Episode 100/100 ------\n"
     ]
    }
   ],
   "source": [
    "#==========Create a q-table==========#\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "q_table = QTable(num_states, num_actions)\n",
    "\n",
    "num_episodes = 100\n",
    "num_timesteps = 128\n",
    "# set the percent you want to explore\n",
    "epsilon = 0.8\n",
    "decrease_step_per_episode = epsilon / num_episodes\n",
    "\n",
    "training_log = []\n",
    "for episode in range(num_episodes):\n",
    "    print(f'Episode {episode+1}/{num_episodes} ', end='', flush=True)\n",
    "\n",
    "    episode_log = []\n",
    "    state = env.reset()\n",
    "    for timestep in range(num_timesteps):\n",
    "        print('-', end='', flush=True)\n",
    "        #==========Taking action: Explore or Exploit==========#\n",
    "\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # Explore: select a random action\n",
    "            action = random.randrange(num_actions)\n",
    "            isRand = True\n",
    "        else:\n",
    "            # Exploit: select the action with max value (future reward)\n",
    "            action = q_table.get_next_action(state)\n",
    "            isRand = False\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done and (new_state in hole_states):\n",
    "            # if fall into hole\n",
    "            mReward = hole_reward\n",
    "        elif done and (new_state in goal_states):\n",
    "            # reach the goal\n",
    "            mReward = goal_reward\n",
    "        else:\n",
    "            # haven't fallen into hole\n",
    "            if state == new_state:\n",
    "                # the agent slams into the wall (not moving to get points)\n",
    "                mReward = -1\n",
    "            else:\n",
    "                # NOTE\n",
    "                # do not give reward to the agent for repeated tasks\n",
    "                # the agent will try to exploit them\n",
    "                mReward = 0\n",
    "\n",
    "        #==========Updating the q-table==========#\n",
    "        q_table.update_table(state, action, mReward, new_state)\n",
    "\n",
    "        episode_log.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'isRand': isRand,\n",
    "            'reward': mReward,\n",
    "            'new_state': new_state\n",
    "        })\n",
    "\n",
    "        state = new_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    epsilon -= decrease_step_per_episode\n",
    "    training_log.append(episode_log)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Q-table (the `agent`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this `cell` repeatedly to observe the `agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 1.0, True, {'prob': 1.0})\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "obs = env.step(q_table.get_next_action(state))\n",
    "print(obs)\n",
    "\n",
    "new_state, reward, done, info = obs\n",
    "state = new_state\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
